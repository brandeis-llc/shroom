{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LQaAfJyiy88"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate sentencepiece accelerate sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r54-jSo7cJMm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, TrainingArguments, Trainer\n",
        "# from sentence_transformers import CrossEncoder\n",
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW, SGD\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkrwleGJfHj1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qKuhvsqd253"
      },
      "outputs": [],
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OA5OHW2mPEZ"
      },
      "outputs": [],
      "source": [
        "dataset = {}\n",
        "df = pd.read_json(\"/content/drive/MyDrive/shroom/val.model-aware.v2.json\")\n",
        "for ind in df.index:\n",
        "  df[\"label\"][ind] = 0 if df[\"label\"][ind]==\"Hallucination\" else 1\n",
        "\n",
        "dataset[\"aware\"] = df\n",
        "\n",
        "df = pd.read_json(\"/content/drive/MyDrive/shroom/val.model-agnostic.json\")\n",
        "for ind in df.index:\n",
        "  df[\"label\"][ind] = 0 if df[\"label\"][ind]==\"Hallucination\" else 1\n",
        "\n",
        "dataset[\"agnostic\"] = df\n",
        "\n",
        "df = pd.read_json(\"/content/drive/MyDrive/shroom/test.model-aware.json\")\n",
        "\n",
        "dataset[\"aware_test\"] = df\n",
        "\n",
        "df = pd.read_json(\"/content/drive/MyDrive/shroom/test.model-agnostic.json\")\n",
        "\n",
        "dataset[\"agnostic_test\"] = df\n",
        "# pandas dataframe into huggingface dataset\n",
        "aware_data = Dataset.from_pandas(dataset[\"aware\"])\n",
        "agnostic_data = Dataset.from_pandas(dataset[\"agnostic\"])\n",
        "aware_test_data = Dataset.from_pandas(dataset[\"aware_test\"])\n",
        "agnostic_test_data = Dataset.from_pandas(dataset[\"agnostic_test\"])\n",
        "\n",
        "print(aware_data)\n",
        "print(agnostic_data)\n",
        "\n",
        "# combine two Datasets into one Dataset not a DatasetDict\n",
        "all_data = pd.concat([dataset[\"aware\"], dataset[\"agnostic\"]])\n",
        "\n",
        "print(all_data)\n",
        "\n",
        "\n",
        "#all_data = all_data.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "#all_data\n",
        "\n",
        "# put the train and eval data into one dataset\n",
        "# dataset = DatasetDict({\"train\": train_data, \"eval\": eval_data})\n",
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_wJLiRTf_Yj"
      },
      "outputs": [],
      "source": [
        "def dump_instances(instances, json_file: str):\n",
        "    json_lst = [ins for ins in instances]\n",
        "    with open(json_file, \"w\") as f:\n",
        "        json.dump(json_lst, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KWjYTWEjHZh"
      },
      "outputs": [],
      "source": [
        "def predict(data):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
        "  scores = model.predict([[data[i][\"tgt\"], data[i][\"hyp\"]] for i in range(len(data))])\n",
        "\n",
        "  correct = 0\n",
        "  for ins in range(len(data)):\n",
        "    if data[ins][\"label\"] == 1 and scores[ins] > 0.5:\n",
        "      correct += 1\n",
        "    elif data[ins][\"label\"] == 0 and scores[ins] < 0.5:\n",
        "      correct += 1\n",
        "\n",
        "  return correct/len(data)\n",
        "\n",
        "print(\"Aware\", predict(aware_data))\n",
        "print(\"Agnostic\", predict(agnostic_data))\n",
        "print(\"All\", predict(all_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJon9oySZ7Xg"
      },
      "outputs": [],
      "source": [
        "def predict_and_output(data):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
        "  scores = model.predict([[data[i][\"tgt\"], data[i][\"hyp\"]] if data[i][\"tgt\"] != \"\" else [data[i][\"src\"], data[i][\"hyp\"]] for i in range(len(data))])\n",
        "\n",
        "  docs = []\n",
        "  for d, score in zip(data, scores):\n",
        "    pred = dict()\n",
        "    pred[\"id\"] = d[\"id\"] if \"id\" in d.keys() else 0\n",
        "    pred[\"p(Hallucination)\"] = 1 - score\n",
        "    pred[\"label\"] = \"Not Hallucination\" if score > 0.5 else \"Hallucination\"\n",
        "    docs.append(pred)\n",
        "\n",
        "  dump_instances(docs, \"/content/drive/MyDrive/shroom/output_4.json\")\n",
        "  return docs\n",
        "\n",
        "\n",
        "a = predict_and_output(aware_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KlhKJVYpLHh"
      },
      "outputs": [],
      "source": [
        "def predict_and_dump_model(model, data, output_file=None):\n",
        "  scores = model.predict([[data[i][\"tgt\"], data[i][\"hyp\"]] if data[i][\"tgt\"] != \"\" else [data[i][\"src\"], data[i][\"hyp\"]] for i in range(len(data))])\n",
        "\n",
        "  docs = []\n",
        "  for d, score in zip(data, scores):\n",
        "    pred = dict()\n",
        "    pred[\"id\"] = d[\"id\"] if \"id\" in d.keys() else 0\n",
        "    pred[\"p(Hallucination)\"] = 1 - score\n",
        "    pred[\"label\"] = \"Not Hallucination\" if score > 0.5 else \"Hallucination\"\n",
        "    docs.append(pred)\n",
        "  if output_file is None:\n",
        "    output_file = \"/content/drive/MyDrive/shroom/output_X.json\"\n",
        "  dump_instances(docs, output_file)\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc0P280YpBli"
      },
      "outputs": [],
      "source": [
        "def train_and_dump(train_data, num_epochs=5, test_data=None, output_file=None, model=None, intermediate=None):\n",
        "  if model is None:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'vectara/hallucination_evaluation_model', trust_remote_code=True)\n",
        "    # model = CrossEncoder('vectara/hallucination_evaluation_model', num_labels=1, automodel_args={'ignore_mismatched_sizes': True})\n",
        "  # manually split data into t_data and test_data after shuffling\n",
        "  if test_data is None and intermediate is None:\n",
        "    t_data, test_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "  else:\n",
        "    t_data = train_data\n",
        "  # create input examples for the model\n",
        "  # if e, use \"src\" instead of \"tgt\" if \"tgt\" is empty\n",
        "  train_examples = [InputExample(texts=[t_data[\"tgt\"][i], t_data[\"hyp\"][i]], label=int(t_data[\"label\"][i])) if t_data[\"tgt\"][i] != \"\" else InputExample(texts=[t_data[\"src\"][i], t_data[\"hyp\"][i]], label=int(t_data[\"label\"][i])) for i in range(len(t_data))]\n",
        "  # test_examples = [InputExample(texts=[test_data[\"tgt\"][i], test_data[\"hyp\"][i]], label=int(test_data[\"label\"][i])) if test_data[\"tgt\"][i] != \"\" else InputExample(texts=[test_data[\"src\"][i], test_data[\"hyp\"][\"i\"]], label=int(test_data[\"label\"][i])) for i in range(len(test_data))]\n",
        "  # test_evaluator = CEBinaryClassificationEvaluator.from_input_examples(test_examples, name='test')\n",
        "\n",
        "  train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE)\n",
        "  # eval_dataloader = DataLoader(test_examples, batch_size=BATCH_SIZE)\n",
        "  warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "  model.fit(train_dataloader=train_dataloader,\n",
        "            epochs=num_epochs,\n",
        "            warmup_steps=warmup_steps,\n",
        "            output_path=f\"/content/drive/MyDrive/shroom/model\",\n",
        "            use_amp=True,\n",
        "            show_progress_bar=True)\n",
        "  if intermediate is None:\n",
        "    predict_and_dump_model(model, test_data, output_file=output_file)\n",
        "  return model\n",
        "\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=3, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_7.json\")\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=4, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_8.json\")\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=5, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_9.json\")\n",
        "train_and_dump(train_data=aware_data, num_epochs=1, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_10.json\")\n",
        "train_and_dump(train_data=aware_data, num_epochs=2, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_11.json\")\n",
        "train_and_dump(train_data=aware_data, num_epochs=3, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_12.json\")\n",
        "train_and_dump(train_data=aware_data, num_epochs=4, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_13.json\")\n",
        "train_and_dump(train_data=aware_data, num_epochs=5, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_14.json\")\n",
        "\n",
        "# joint training\n",
        "\n",
        "train_and_dump(train_data=all_data, num_epochs=1, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_15.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=2, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_16.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=3, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_17.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=4, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_18.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=5, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_19.json\")\n",
        "\n",
        "train_and_dump(train_data=all_data, num_epochs=1, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_20.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=2, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_21.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=3, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_22.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=4, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_23.json\")\n",
        "train_and_dump(train_data=all_data, num_epochs=5, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_24.json\")\n",
        "\n",
        "a = train_and_dump(train_data=all_data, num_epochs=3, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=1, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_25.json\", model=a)\n",
        "\n",
        "b = train_and_dump(train_data=all_data, num_epochs=3, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=aware_data, num_epochs=1, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_26.json\", model=b)\n",
        "\n",
        "c = train_and_dump(train_data=all_data, num_epochs=5, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=1, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_27.json\", model=c)\n",
        "\n",
        "d = train_and_dump(train_data=all_data, num_epochs=5, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=aware_data, num_epochs=1, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_28.json\", model=d)\n",
        "\n",
        "e = train_and_dump(train_data=all_data, num_epochs=1, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=agnostic_data, num_epochs=1, test_data=agnostic_test_data, output_file=\"/content/drive/MyDrive/shroom/output_29.json\", model=e)\n",
        "\n",
        "f = train_and_dump(train_data=all_data, num_epochs=1, test_data=None, output_file=\"/content/drive/MyDrive/shroom/ignore.json\", intermediate=True)\n",
        "train_and_dump(train_data=aware_data, num_epochs=1, test_data=aware_test_data, output_file=\"/content/drive/MyDrive/shroom/output_30.json\", model=f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DuhaThYpZ45"
      },
      "outputs": [],
      "source": [
        "def predict_model(model, data):\n",
        "  scores = model.predict([[data[\"tgt\"][i], data[\"hyp\"][i]] for i in data.index])\n",
        "\n",
        "  correct = 0\n",
        "  for ins, s_ins in zip(data.index, range(len(scores))):\n",
        "    if data[\"label\"][ins] == 1 and scores[s_ins] > 0.5:\n",
        "      correct += 1\n",
        "    elif data[\"label\"][ins] == 0 and scores[s_ins] < 0.5:\n",
        "      correct += 1\n",
        "\n",
        "  print(correct/len(data))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}